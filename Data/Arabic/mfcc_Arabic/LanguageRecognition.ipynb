{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ae88f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model for Ar_F10_mfcc saved successfully.\n",
      "Model for Ar_F12_mfcc saved successfully.\n",
      "Model for Ar_F16_mfcc saved successfully.\n",
      "Model for Ar_H2_mfcc saved successfully.\n",
      "Model for Ar_H3_mfcc saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "l=['Ar_F10_mfcc','Ar_F12_mfcc', 'Ar_F16_mfcc', 'Ar_H2_mfcc', 'Ar_H3_mfcc']\n",
    "models = {}\n",
    "\n",
    "for i in l:\n",
    "    # Path to the folder containing MFCC files\n",
    "    folder_path = rf'C:\\Users\\Pro\\Desktop\\speech processing\\speeshdata\\english_data\\Data\\Arabic\\mfcc_Arabic\\{i}'\n",
    "    files = os.listdir(folder_path)\n",
    "    # Initialize an empty array to store all MFCC data\n",
    "    all_mfcc_data = []\n",
    "\n",
    "    # Iterate over each file\n",
    "    for file_name in files:\n",
    "        # Read the MFCC data from the file\n",
    "        with open(os.path.join(folder_path, file_name), 'r') as file:\n",
    "            mfcc_data = np.loadtxt(file)\n",
    "            # Append the MFCC data to the list\n",
    "            all_mfcc_data.append(mfcc_data)\n",
    "\n",
    "    # Concatenate all MFCC data into a single array\n",
    "    all_mfcc_data = np.concatenate(all_mfcc_data)\n",
    "\n",
    "    # Determine the number of features\n",
    "    num_features = all_mfcc_data.shape[1]\n",
    "\n",
    "    gmm_16 = GaussianMixture(n_components=16, covariance_type='full', tol=1e-3, reg_covar=1e-6, max_iter=100, n_init=10, init_params='kmeans', random_state=0)\n",
    "\n",
    "    gmm_16.fit(all_mfcc_data)\n",
    "    models[i] = gmm_16\n",
    "    model_file_path = f'gmm_{i}_16.pkl'\n",
    "    # Save the model to the specified file path\n",
    "    joblib.dump(gmm_16, model_file_path)\n",
    "    print(f\"Model for {i} saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ddb1b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model for Ar_F10_mfcc saved successfully.\n",
      "Model for Ar_F12_mfcc saved successfully.\n",
      "Model for Ar_F16_mfcc saved successfully.\n",
      "Model for Ar_H2_mfcc saved successfully.\n",
      "Model for Ar_H3_mfcc saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "l=['Ar_F10_mfcc','Ar_F12_mfcc', 'Ar_F16_mfcc', 'Ar_H2_mfcc', 'Ar_H3_mfcc']\n",
    "models = {}\n",
    "\n",
    "for i in l:\n",
    "    # Path to the folder containing MFCC files\n",
    "    folder_path = rf'C:\\Users\\Pro\\Desktop\\speech processing\\speeshdata\\english_data\\Data\\Arabic\\mfcc_Arabic\\{i}'\n",
    "\n",
    "    files = os.listdir(folder_path)\n",
    "\n",
    "    # Initialize an empty array to store all MFCC data\n",
    "    all_mfcc_data = []\n",
    "\n",
    "    # Iterate over each file\n",
    "    for file_name in files:\n",
    "        # Read the MFCC data from the file\n",
    "        with open(os.path.join(folder_path, file_name), 'r') as file:\n",
    "            mfcc_data = np.loadtxt(file)\n",
    "            # Append the MFCC data to the list\n",
    "            all_mfcc_data.append(mfcc_data)\n",
    "\n",
    "    # Concatenate all MFCC data into a single array\n",
    "    all_mfcc_data = np.concatenate(all_mfcc_data)\n",
    "\n",
    "    # Determine the number of features\n",
    "    num_features = all_mfcc_data.shape[1]\n",
    "\n",
    "    gmm_32 = GaussianMixture(n_components=32, covariance_type='full', tol=1e-3, reg_covar=1e-6, max_iter=100, n_init=10, init_params='kmeans', random_state=0)\n",
    "\n",
    "    gmm_32.fit(all_mfcc_data)\n",
    "    models[i] = gmm_32\n",
    "    model_file_path = f'gmm_{i}_32.pkl'\n",
    "    # Save the model to the specified file path\n",
    "    joblib.dump(gmm_32, model_file_path)\n",
    "    print(f\"Model for {i} saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f9857d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model for Ar_F10_mfcc saved successfully.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 34\u001b[0m\n\u001b[0;32m     30\u001b[0m num_features \u001b[38;5;241m=\u001b[39m all_mfcc_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     32\u001b[0m gmm_256 \u001b[38;5;241m=\u001b[39m GaussianMixture(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, covariance_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m'\u001b[39m, tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, reg_covar\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, n_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, init_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkmeans\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m \u001b[43mgmm_256\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_mfcc_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m models[i] \u001b[38;5;241m=\u001b[39m gmm_256\n\u001b[0;32m     36\u001b[0m model_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgmm_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_256.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\mixture\\_base.py:186\u001b[0m, in \u001b[0;36mBaseMixture.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"Estimate model parameters with the EM algorithm.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03mThe method fits the model ``n_init`` times and sets the parameters with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m    The fitted mixture.\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# parameters are validated in fit_predict\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\mixture\\_base.py:253\u001b[0m, in \u001b[0;36mBaseMixture.fit_predict\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    250\u001b[0m prev_lower_bound \u001b[38;5;241m=\u001b[39m lower_bound\n\u001b[0;32m    252\u001b[0m log_prob_norm, log_resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_e_step(X)\n\u001b[1;32m--> 253\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_resp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m lower_bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_lower_bound(log_resp, log_prob_norm)\n\u001b[0;32m    256\u001b[0m change \u001b[38;5;241m=\u001b[39m lower_bound \u001b[38;5;241m-\u001b[39m prev_lower_bound\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\mixture\\_gaussian_mixture.py:754\u001b[0m, in \u001b[0;36mGaussianMixture._m_step\u001b[1;34m(self, X, log_resp)\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_m_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, log_resp):\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;124;03m\"\"\"M step.\u001b[39;00m\n\u001b[0;32m    744\u001b[0m \n\u001b[0;32m    745\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;124;03m        the point of each sample in X.\u001b[39;00m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    753\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeans_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariances_ \u001b[38;5;241m=\u001b[39m _estimate_gaussian_parameters(\n\u001b[1;32m--> 754\u001b[0m         X, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_resp\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreg_covar, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariance_type\n\u001b[0;32m    755\u001b[0m     )\n\u001b[0;32m    756\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_ \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecisions_cholesky_ \u001b[38;5;241m=\u001b[39m _compute_precision_cholesky(\n\u001b[0;32m    758\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariances_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovariance_type\n\u001b[0;32m    759\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "l=['Ar_F10_mfcc','Ar_F12_mfcc', 'Ar_F16_mfcc', 'Ar_H2_mfcc', 'Ar_H3_mfcc']\n",
    "models = {}\n",
    "\n",
    "for i in l:\n",
    "    # Path to the folder containing MFCC files\n",
    "    folder_path = rf'C:\\Users\\Pro\\Desktop\\speech processing\\speeshdata\\english_data\\Data\\Arabic\\mfcc_Arabic\\{i}'\n",
    "\n",
    "    files = os.listdir(folder_path)\n",
    "\n",
    "    # Initialize an empty array to store all MFCC data\n",
    "    all_mfcc_data = []\n",
    "\n",
    "    # Iterate over each file\n",
    "    for file_name in files:\n",
    "        # Read the MFCC data from the file\n",
    "        with open(os.path.join(folder_path, file_name), 'r') as file:\n",
    "            mfcc_data = np.loadtxt(file)\n",
    "            # Append the MFCC data to the list\n",
    "            all_mfcc_data.append(mfcc_data)\n",
    "\n",
    "    # Concatenate all MFCC data into a single array\n",
    "    all_mfcc_data = np.concatenate(all_mfcc_data)\n",
    "\n",
    "    # Determine the number of features\n",
    "    num_features = all_mfcc_data.shape[1]\n",
    "\n",
    "    gmm_256 = GaussianMixture(n_components=256, covariance_type='full', tol=1e-3, reg_covar=1e-6, max_iter=100, n_init=10, init_params='kmeans', random_state=0)\n",
    "\n",
    "    gmm_256.fit(all_mfcc_data)\n",
    "    models[i] = gmm_256\n",
    "    model_file_path = f'gmm_{i}_256.pkl'\n",
    "    # Save the model to the specified file path\n",
    "    joblib.dump(gmm_256, model_file_path)\n",
    "    print(f\"Model for {i} saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67014f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "l=['Ar_F10_mfcc','Ar_F12_mfcc', 'Ar_F16_mfcc', 'Ar_H2_mfcc', 'Ar_H3_mfcc']\n",
    "models_16= {}\n",
    "for i in l:\n",
    "    model_file_path = f'gmm_{i}_16.pkl'\n",
    "    # Load the model from the file\n",
    "    loaded_model = joblib.load(model_file_path)\n",
    "    models_16[i] = loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9357022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ar_F10_mfcc': GaussianMixture(n_components=16, n_init=10, random_state=0),\n",
       " 'Ar_F12_mfcc': GaussianMixture(n_components=16, n_init=10, random_state=0),\n",
       " 'Ar_F16_mfcc': GaussianMixture(n_components=16, n_init=10, random_state=0),\n",
       " 'Ar_H2_mfcc': GaussianMixture(n_components=16, n_init=10, random_state=0),\n",
       " 'Ar_H3_mfcc': GaussianMixture(n_components=16, n_init=10, random_state=0)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899d1d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "l=['Ar_F10_mfcc','Ar_F12_mfcc', 'Ar_F16_mfcc', 'Ar_H2_mfcc', 'Ar_H3_mfcc']\n",
    "accuracy_16=[]\n",
    "for i in l:\n",
    "    # Path to the folder containing the test set MFCC files\n",
    "    test_folder_path = rf'C:\\Users\\Pro\\Desktop\\speech processing\\speeshdata\\english_data\\Data\\{i}\\test_{i}'\n",
    "\n",
    "    # List all files in the test folder\n",
    "    test_files = os.listdir(test_folder_path)\n",
    "\n",
    "    num_correct = 0\n",
    "    num_total = len(test_files)\n",
    "\n",
    "    for file_name in test_files:\n",
    "        # Read the MFCC data from the file\n",
    "        with open(os.path.join(test_folder_path, file_name), 'r') as file:\n",
    "            test_mfcc_data = np.loadtxt(file)\n",
    "\n",
    "        max_score = float('-inf')\n",
    "        best_model = None\n",
    "\n",
    "        for language, model in models.items():\n",
    "            score = model.score(test_mfcc_data)\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                best_model = language\n",
    "\n",
    "        if best_model == i:\n",
    "            num_correct += 1\n",
    "\n",
    "    accuracy = num_correct / num_total\n",
    "    accuracy_16.append(accuracy)\n",
    "    \n",
    "    print(f\"Accuracy of the {i} model on the test set: {accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
